{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3507e-5ea4-4fe0-a937-65eb24f03664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name-Atharva Talegaonkar SID-862467125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c37370-6a01-4a15-8a11-3ec75ff5a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features.to_numpy()\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets.to_numpy()\n",
    "  \n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cebdaa-b767-4039-a411-f974f84ea729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question -1  Implementing simple classifiers\n",
    "\n",
    "Here i am first implementing decision tree , because in decision tree they split the nodes recursively until only pure nodes are left i.e.\n",
    "data belonging to the same groups are left on each side , data segregation is done here.The splitting of data is done based on the condition \n",
    "at the root node. This process keeps on happening until data is fully seperated according to their groups.Firstly, I am splitting the data with 20% \n",
    "using for testing and 80% for training purpose.Then I made a decision tree from scratch containing various functions.Next I implemented Naive Bayes \n",
    "Classifier with Gaussian modeling .It is a machine learning model with probablistic approach.I also implemented this from scratch. After defining both \n",
    "the classifiers , I evaluated both using stratified 10-fold cross-validation.Then I plotted comparison of both the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c4a2af-ed03-492b-9eab-e07df851e1de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\EndUser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\EndUser\\AppData\\Local\\Temp\\ipykernel_13728\\2873932603.py\", line 111, in fit\n    X_c = X[y == c]\n          ~^^^^^^^^\nIndexError: boolean index did not match indexed array along dimension 1; dimension is 30 but corresponding boolean dimension is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m dt_mean_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(dt_f1_scores)\n\u001b[0;32m    139\u001b[0m dt_std_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(dt_f1_scores)\n\u001b[1;32m--> 141\u001b[0m nb_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluation_of_Classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m nb_mean_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(nb_f1_scores)\n\u001b[0;32m    143\u001b[0m nb_std_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(nb_f1_scores) \n",
      "Cell \u001b[1;32mIn[9], line 131\u001b[0m, in \u001b[0;36mEvaluation_of_Classifier\u001b[1;34m(clf, X, y)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEvaluation_of_Classifier\u001b[39m(clf, X, y):\n\u001b[0;32m    130\u001b[0m     skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 131\u001b[0m     f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1_macro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f1_scores\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:450\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    430\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    431\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    432\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    448\u001b[0m )\n\u001b[1;32m--> 450\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\EndUser\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\EndUser\\AppData\\Local\\Temp\\ipykernel_13728\\2873932603.py\", line 111, in fit\n    X_c = X[y == c]\n          ~^^^^^^^^\nIndexError: boolean index did not match indexed array along dimension 1; dimension is 30 but corresponding boolean dimension is 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class Decision_Tree_Information_Gain:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self.growth_of_tree(X, y)\n",
    "\n",
    "    def growth_of_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        \n",
    "        if (self.max_depth is not None and depth == self.max_depth) or n_labels == 1:\n",
    "            return Node(value=self.Most_Common_Label(y))\n",
    "\n",
    "        Best_Split = self.search_best_split(X, y)\n",
    "        if Best_Split is None:\n",
    "            return Node(value=self.Most_Common_Label(y))\n",
    "        \n",
    "        all_left_indices = np.where(X[:, Best_Split['feature']] <= Best_Split['threshold'])[0]\n",
    "        all_right_indices = np.where(X[:, Best_Split['feature']] > Best_Split['threshold'])[0]\n",
    "\n",
    "        Left_tree = self.growth_of_tree(X[all_left_indices], y[all_left_indices], depth + 1)\n",
    "        Right_tree = self.growth_of_tree(X[all_right_indices], y[all_right_indices], depth + 1)\n",
    "\n",
    "        return Node(feature=Best_Split['feature'], threshold=Best_Split['threshold'],\n",
    "                    left=Left_tree, right=Right_tree)\n",
    "\n",
    "    def search_best_split(self, X, y):\n",
    "        Best_Split = None\n",
    "        Best_Info_Gain = -1\n",
    "\n",
    "        for feature in range(self.n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                all_left_indices = np.where(X[:, feature] <= threshold)[0]\n",
    "                all_right_indices = np.where(X[:, feature] > threshold)[0]\n",
    "\n",
    "                if len(all_left_indices) > 0 and len(all_right_indices) > 0:\n",
    "                    info_gain = self.Information_gained(y, y[all_left_indices], y[all_right_indices])\n",
    "                    if info_gain > Best_Info_Gain:\n",
    "                        Best_Split = {'feature': feature, 'threshold': threshold}\n",
    "                        Best_Info_Gain = info_gain\n",
    "\n",
    "        return Best_Split\n",
    "\n",
    "    def Information_gained(self, parent, left_child, right_child):\n",
    "        p = len(parent)\n",
    "        pl = len(left_child)\n",
    "        pr = len(right_child)\n",
    "        \n",
    "        return self.Entropy(parent) - (pl/p) * self.Entropy(left_child) - (pr/p) * self.Entropy(right_child)\n",
    "\n",
    "    def Entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "    def Most_Common_Label(self, y):\n",
    "        labels, counts = np.unique(y, return_counts=True)\n",
    "        return labels[np.argmax(counts)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.Prediction_Tree(x, self.tree) for x in X])\n",
    "\n",
    "    def Prediction_Tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.Prediction_Tree(x, node.left)\n",
    "        else:\n",
    "            return self.Prediction_Tree(x, node.right)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'max_depth': self.max_depth}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class Naive_Bayes_Classifier_Gaussian_Modeling:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = np.zeros((len(self.classes), n_features))\n",
    "        self.stds = np.zeros((len(self.classes), n_features))\n",
    "        self.priors = np.zeros(len(self.classes))\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.means[i] = X_c.mean(axis=0)\n",
    "            self.stds[i] = X_c.std(axis=0)\n",
    "            self.priors[i] = len(X_c) / n_samples\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = np.zeros((X.shape[0], len(self.classes)))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            probs[:, i] = np.prod(self._gaussian_pdf(X, self.means[i], self.stds[i]), axis=1) * self.priors[i]\n",
    "        return self.classes[np.argmax(probs, axis=1)]\n",
    "\n",
    "    def _gaussian_pdf(self, X, mean, std):\n",
    "        exponent = np.exp(-(X - mean) * 2 / (2 * std * 2))\n",
    "        return (1 / (np.sqrt(2 * np.pi) * std)) * exponent\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "def Evaluation_of_Classifier(clf, X, y):\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    f1_scores = cross_val_score(clf, X, y, cv=skf, scoring='f1_macro')\n",
    "    return f1_scores\n",
    "\n",
    "dt_classifier = Decision_Tree_Information_Gain()\n",
    "nb_classifier = Naive_Bayes_Classifier_Gaussian_Modeling()\n",
    "\n",
    "dt_f1_scores = Evaluation_of_Classifier(dt_classifier, X, y)\n",
    "dt_mean_f1 = np.mean(dt_f1_scores)\n",
    "dt_std_f1 = np.std(dt_f1_scores)\n",
    "\n",
    "nb_f1_scores = Evaluation_of_Classifier(nb_classifier, X, y)\n",
    "nb_mean_f1 = np.mean(nb_f1_scores)\n",
    "nb_std_f1 = np.std(nb_f1_scores) \n",
    "\n",
    "classifiers = ['Decision Tree', 'Naive Bayes']\n",
    "mean_f1_scores = [dt_mean_f1, nb_mean_f1]\n",
    "std_f1_scores = [dt_std_f1, nb_std_f1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(classifiers, mean_f1_scores, yerr=std_f1_scores, capsize=5, color=['red', 'purple'])\n",
    "\n",
    "for bar, mean_f1 in zip(bars, mean_f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02, f'{mean_f1:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('F1 Score (Avg ± Std)')\n",
    "plt.title('Performance Comparison of Classifiers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5226291-33a4-416d-9cc6-e5e6625761f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question - 2  Dimensionality reduction with the Singular Value Decomposition\n",
    "Here I am implementing dimensioanlity reduction using SVD to get the best axis to project on, meaning best means minimum sum of sqaures of \n",
    "projection error .For this implementation , I am using truncated SVD because after much research , I found out that Truncated SVD works very well \n",
    "with both sparse and dense data. Basically , here I did matrix multiplication X=UΣV(t). U is the matrix containing left singular vectors ,Σ is a matrix \n",
    "that contain top k values , through which we are eliminating the smallest singular value and V(t) is the tranpose of matrix V conitaning right singular\n",
    "vectors.I have generated the Truncated SVD from scratch using different matrix related operations.We are eliminating left right most  column from U and \n",
    "bottom most row from V(t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e6f51d4-ea44-4192-856a-f100fb653f32",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 2 but corresponding boolean dimension is 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m dt_mean_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, dt_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Fitting of Naive Bayes Classifier\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[43mnb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m nb_predictions \u001b[38;5;241m=\u001b[39m nb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test_reduced)\n\u001b[0;32m    163\u001b[0m nb_mean_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, nb_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 118\u001b[0m, in \u001b[0;36mNaive_Bayes_Classifier_Gaussian_Modeling.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_indices \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)}\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_priors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mmean(y \u001b[38;5;241m==\u001b[39m c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses])\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_means \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses]\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_vars \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mvar(X[y \u001b[38;5;241m==\u001b[39m c], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses]\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 2 but corresponding boolean dimension is 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you already have X and y loaded\n",
    "# Example: X, y = make_classification(...)\n",
    "\n",
    "# Splitting data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implementing truncated svd from scratch\n",
    "def Truncated_Svd(X, k):\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    Covariance_Matrix = np.dot(X_centered.T, X_centered)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(Covariance_Matrix)\n",
    "    Top_k_Indices = np.argsort(eigenvalues)[::-1][:k]\n",
    "    U = eigenvectors[:, Top_k_Indices]\n",
    "    S = np.sqrt(np.abs(eigenvalues[Top_k_Indices]))\n",
    "    Vt = np.dot(U.T, X_centered.T) / S[:, np.newaxis]\n",
    "    return U, S, Vt\n",
    "\n",
    "# Applying SVD for dimensionality reduction\n",
    "def Applying_SVD(X_train, X_test, rank):\n",
    "    U_train, _, Vt_train = Truncated_Svd(X_train, rank)\n",
    "    X_train_reduced = np.dot(X_train, U_train)\n",
    "    X_test_reduced = np.dot(X_test, U_train)\n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "class Decision_Tree_Information_Gain:\n",
    "    def __init__(self, max_depth=None): \n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self.growth_of_tree(X, y)\n",
    "        \n",
    "    def growth_of_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        if (self.max_depth is not None and depth == self.max_depth) or n_labels == 1:\n",
    "            return Node(value=self.Most_Common_Label(y))\n",
    "        Best_Split = self.search_best_split(X, y)\n",
    "        if Best_Split is None:\n",
    "            return Node(value=self.Most_Common_Label(y))\n",
    "        all_left_indices = np.where(X[:, Best_Split['feature']] <= Best_Split['threshold'])[0]\n",
    "        all_right_indices = np.where(X[:, Best_Split['feature']] > Best_Split['threshold'])[0]\n",
    "        Left_tree = self.growth_of_tree(X[all_left_indices], y[all_left_indices], depth + 1)\n",
    "        Right_tree = self.growth_of_tree(X[all_right_indices], y[all_right_indices], depth + 1)\n",
    "        return Node(feature=Best_Split['feature'], threshold=Best_Split['threshold'], left=Left_tree, right=Right_tree)\n",
    "        \n",
    "    def search_best_split(self, X, y):\n",
    "        Best_Split = None\n",
    "        Best_Info_Gain = -1\n",
    "        for feature in range(self.n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                all_left_indices = np.where(X[:, feature] <= threshold)[0]\n",
    "                all_right_indices = np.where(X[:, feature] > threshold)[0]\n",
    "                if len(all_left_indices) > 0 and len(all_right_indices) > 0:\n",
    "                    info_gain = self.Information_gained(y, y[all_left_indices], y[all_right_indices])\n",
    "                    if info_gain > Best_Info_Gain:\n",
    "                        Best_Split = {'feature': feature, 'threshold': threshold}\n",
    "                        Best_Info_Gain = info_gain\n",
    "        return Best_Split\n",
    "\n",
    "    def Information_gained(self, parent, left_child, right_child):\n",
    "        p = len(parent)\n",
    "        pl = len(left_child)\n",
    "        pr = len(right_child)\n",
    "        return self.Entropy(parent) - (pl/p) * self.Entropy(left_child) - (pr/p) * self.Entropy(right_child)\n",
    "\n",
    "    def Entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "\n",
    "    def Most_Common_Label(self, y):\n",
    "        labels, counts = np.unique(y, return_counts=True)\n",
    "        return labels[np.argmax(counts)]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.Prediction_Tree(x, self.tree) for x in X])\n",
    "\n",
    "    def Prediction_Tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.Prediction_Tree(x, node.left)\n",
    "        else:\n",
    "            return self.Prediction_Tree(x, node.right)\n",
    "            \n",
    "    def get_params(self, deep=True):\n",
    "        return {'max_depth': self.max_depth}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, value=None, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class Naive_Bayes_Classifier_Gaussian_Modeling:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_indices = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.class_priors = np.array([np.mean(y == c) for c in self.classes])\n",
    "        self.class_means = [np.mean(X[y == c], axis=0) for c in self.classes]\n",
    "        self.class_vars = [np.var(X[y == c], axis=0) for c in self.classes]\n",
    "\n",
    "    def Gaussian_Density(self, x, mean, var):\n",
    "        return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-(x - mean)**2 / (2 * var))\n",
    "\n",
    "    def Predicting_Class_Probas(self, x):\n",
    "        class_probs = []\n",
    "        for cls in self.classes:\n",
    "            idx = self.class_indices[cls]\n",
    "            prior = self.class_priors[idx]\n",
    "            likelihood = np.prod(self.Gaussian_Density(x, self.class_means[idx], self.class_vars[idx]))\n",
    "            class_probs.append(prior * likelihood)\n",
    "        return class_probs / np.sum(class_probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.classes[np.argmax(self.Predicting_Class_Probas(x))] for x in X])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        return self\n",
    "\n",
    "# Listing of SVD ranks for evaluation in the line graph.\n",
    "svd_ranks = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Dictionary to store the performance of both the classifiers.\n",
    "classifier_results = {'Decision Tree': [], 'Naive Bayes': []}\n",
    "\n",
    "# Evaluation of both the classifiers for each SVD rank.\n",
    "for rank in svd_ranks:\n",
    "    X_train_reduced, X_test_reduced = Applying_SVD(X_train, X_test, rank)\n",
    "    \n",
    "    dt_classifier = Decision_Tree_Information_Gain()\n",
    "    nb_classifier = Naive_Bayes_Classifier_Gaussian_Modeling()\n",
    "    \n",
    "    # Fitting of decision Tree Classifier\n",
    "    dt_classifier.fit(X_train_reduced, y_train)\n",
    "    dt_predictions = dt_classifier.predict(X_test_reduced)\n",
    "    dt_mean_f1 = f1_score(y_test, dt_predictions, average='weighted')\n",
    "    \n",
    "    # Fitting of Naive Bayes Classifier\n",
    "    nb_classifier.fit(X_train_reduced, y_train)\n",
    "    nb_predictions = nb_classifier.predict(X_test_reduced)\n",
    "    nb_mean_f1 = f1_score(y_test, nb_predictions, average='weighted')\n",
    "    \n",
    "    # Storing of results\n",
    "    classifier_results['Decision Tree'].append(dt_mean_f1)\n",
    "    classifier_results['Naive Bayes'].append(nb_mean_f1)\n",
    "\n",
    "# Plotting the comparison of performance of both classifiers in the line graph.\n",
    "plt.figure(figsize=(8, 4))\n",
    "for classifier, results in classifier_results.items():\n",
    "    plt.plot(svd_ranks, results, label=classifier, marker='o')\n",
    "\n",
    "plt.xlabel('SVD Rank')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Performance Comparison of Classifiers with SVD')\n",
    "plt.xticks(svd_ranks)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b52ad-0241-4f00-b9a5-73a4b00118e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question - 3  Feature selection with randomization : \n",
    "In this question , I implemented feature selection technique with randomization to determine which features are of more important by measuring the \n",
    "model performance by k-fold cross validaition.The features are selected are random here and the values are permuted here without making any changes in\n",
    "other features.In this way , an importance of feature can be determines by comparions of model with or without that selected feature.Also the importance \n",
    "of any specific feature is found out by fidning degree of performance degradation cause by the randomization of selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1451644c-f584-42e7-b6ae-b2522cd98b60",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m     X_train_fold_randomized \u001b[38;5;241m=\u001b[39m X_train_fold\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     44\u001b[0m     X_train_fold_randomized[:, feature_idx] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(X_train_fold[:, feature_idx])\n\u001b[1;32m---> 45\u001b[0m     f1_randomized \u001b[38;5;241m=\u001b[39m \u001b[43mTraining_AND_Evaluatation_Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_fold_randomized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     f1_randomized_scores\u001b[38;5;241m.\u001b[39mappend(f1_randomized)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Calculate average F1 scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m, in \u001b[0;36mTraining_AND_Evaluatation_Model\u001b[1;34m(model, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTraining_AND_Evaluatation_Model\u001b[39m(model, X_train, y_train, X_test, y_test):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f1_score(y_test, y_pred, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mDecision_Tree_Information_Gain.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y))\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrowth_of_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 51\u001b[0m, in \u001b[0;36mDecision_Tree_Information_Gain.growth_of_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     49\u001b[0m all_left_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X[:, Best_Split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m Best_Split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     50\u001b[0m all_right_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X[:, Best_Split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m>\u001b[39m Best_Split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m Left_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrowth_of_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_left_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_left_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m Right_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrowth_of_tree(X[all_right_indices], y[all_right_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(feature\u001b[38;5;241m=\u001b[39mBest_Split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m], threshold\u001b[38;5;241m=\u001b[39mBest_Split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m], left\u001b[38;5;241m=\u001b[39mLeft_tree, right\u001b[38;5;241m=\u001b[39mRight_tree)\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mDecision_Tree_Information_Gain.growth_of_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth) \u001b[38;5;129;01mor\u001b[39;00m n_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMost_Common_Label(y))\n\u001b[1;32m---> 46\u001b[0m Best_Split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Best_Split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMost_Common_Label(y))\n",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m, in \u001b[0;36mDecision_Tree_Information_Gain.search_best_split\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     62\u001b[0m all_right_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(X[:, feature] \u001b[38;5;241m>\u001b[39m threshold)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_left_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_right_indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m     info_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInformation_gained\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_left_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_right_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info_gain \u001b[38;5;241m>\u001b[39m Best_Info_Gain:\n\u001b[0;32m     66\u001b[0m         Best_Split \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m: feature, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m: threshold}\n",
      "Cell \u001b[1;32mIn[10], line 74\u001b[0m, in \u001b[0;36mDecision_Tree_Information_Gain.Information_gained\u001b[1;34m(self, parent, left_child, right_child)\u001b[0m\n\u001b[0;32m     72\u001b[0m pl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(left_child)\n\u001b[0;32m     73\u001b[0m pr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(right_child)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEntropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (pl\u001b[38;5;241m/\u001b[39mp) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEntropy(left_child) \u001b[38;5;241m-\u001b[39m (pr\u001b[38;5;241m/\u001b[39mp) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEntropy(right_child)\n",
      "Cell \u001b[1;32mIn[10], line 77\u001b[0m, in \u001b[0;36mDecision_Tree_Information_Gain.Entropy\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEntropy\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m---> 77\u001b[0m     _, counts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(probabilities \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(probabilities \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:363\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    361\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (inv_idx,)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_counts:\n\u001b[1;32m--> 363\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mdiff(idx),)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'Decision Tree': Decision_Tree_Information_Gain(),\n",
    "    'Naive Bayes': Naive_Bayes_Classifier_Gaussian_Modeling()\n",
    "}\n",
    "\n",
    "# Training and evaluation function\n",
    "def Training_AND_Evaluatation_Model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return f1_score(y_test, y_pred, pos_label='M')  # Specify pos_label\n",
    "\n",
    "# Create a sample split of the data\n",
    "X_fs_sample, X_holdout, y_fs_sample, y_holdout = train_test_split(X, y, test_size=0.8, stratify=y, random_state=42)\n",
    "\n",
    "# Dictionary for storing performance and feature importance\n",
    "Importance_Feature_Dictionary = {}\n",
    "top_feature_performance_dict = {}\n",
    "\n",
    "# Calculate classifier performance and feature importance\n",
    "for model_name, model in classifiers.items():\n",
    "    feature_importance = []\n",
    "\n",
    "    # Iterate through each feature\n",
    "    for feature_idx in range(X_fs_sample.shape[1]):\n",
    "        X_fs_randomized = X_fs_sample.copy()\n",
    "        np.random.shuffle(X_fs_randomized[:, feature_idx])\n",
    "\n",
    "        # Cross-validation setup\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        f1_actual_scores = []\n",
    "        f1_randomized_scores = []\n",
    "\n",
    "        # Perform cross-validation\n",
    "        for train_idx, test_idx in cv.split(X_fs_sample, y_fs_sample):\n",
    "            X_train_fold, X_test_fold = X_fs_sample[train_idx], X_fs_sample[test_idx]\n",
    "            y_train_fold, y_test_fold = y_fs_sample[train_idx], y_fs_sample[test_idx]\n",
    "\n",
    "            # Train and evaluate with actual data\n",
    "            f1_actual = Training_AND_Evaluatation_Model(model, X_train_fold, y_train_fold, X_test_fold, y_test_fold)\n",
    "            f1_actual_scores.append(f1_actual)\n",
    "\n",
    "            # Train and evaluate with randomized feature data\n",
    "            X_train_fold_randomized = X_train_fold.copy()\n",
    "            X_train_fold_randomized[:, feature_idx] = np.random.permutation(X_train_fold[:, feature_idx])\n",
    "            f1_randomized = Training_AND_Evaluatation_Model(model, X_train_fold_randomized, y_train_fold, X_test_fold, y_test_fold)\n",
    "            f1_randomized_scores.append(f1_randomized)\n",
    "\n",
    "        # Calculate average F1 scores\n",
    "        avg_f1_actual = np.mean(f1_actual_scores)\n",
    "        avg_f1_randomized = np.mean(f1_randomized_scores)\n",
    "\n",
    "        # Calculate performance drop percentage\n",
    "        percentage_drop = ((avg_f1_actual - avg_f1_randomized) / avg_f1_actual) * 100\n",
    "\n",
    "        # Store performance drop percentage\n",
    "        feature_importance.append((feature_idx, percentage_drop))\n",
    "\n",
    "    # Sort features by importance\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    Importance_Feature_Dictionary[model_name] = feature_importance\n",
    "\n",
    "    # Extract feature names and importance values for plotting\n",
    "    feature_names = [f'feature_{idx}' for idx, _ in feature_importance]\n",
    "    importance = [importance for _, importance in feature_importance]\n",
    "\n",
    "    # Store feature names and importance values\n",
    "    Importance_Feature_Dictionary[model_name] = (feature_names, importance)\n",
    "\n",
    "    # Evaluate classifier performance using top 1 to top 10 features\n",
    "    top_feature_performance = []\n",
    "    \n",
    "    for num_features in range(1, 11):\n",
    "        top_features = [feature[0] for feature in feature_importance[:num_features]]\n",
    "        X_holdout_selected = X_holdout[:, top_features]\n",
    "\n",
    "        # Train and evaluate model with top selected features\n",
    "        f1_holdout = Training_AND_Evaluatation_Model(model, X_fs_sample[:, top_features], y_fs_sample, X_holdout_selected, y_holdout)\n",
    "        top_feature_performance.append(f1_holdout)\n",
    "\n",
    "    top_feature_performance_dict[model_name] = top_feature_performance\n",
    "\n",
    "# Plot combined feature importance bar chart for both classifiers\n",
    "plt.figure(figsize=(8, 4))\n",
    "dt_feature_names = Importance_Feature_Dictionary['Decision Tree'][0][:10]\n",
    "dt_importance = Importance_Feature_Dictionary['Decision Tree'][1][:10]\n",
    "\n",
    "# Get the top 10 features and their importance for Naive Bayes\n",
    "nb_feature_names = Importance_Feature_Dictionary['Naive Bayes'][0][:10]\n",
    "nb_importance = Importance_Feature_Dictionary['Naive Bayes'][1][:10]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(10)\n",
    "\n",
    "# Bar chart for Decision Tree classifier\n",
    "plt.bar(index, dt_importance, bar_width, label='Decision Tree')\n",
    "\n",
    "# Bar chart for Naive Bayes classifier\n",
    "plt.bar(index + bar_width, nb_importance, bar_width, label='Naive Bayes')\n",
    "\n",
    "plt.xlabel('Top 10 Feature Names')\n",
    "plt.ylabel('Importance (Percentage Drop in F1 Score)')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.xticks(index + bar_width / 2, dt_feature_names, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot combined classifier performance with top features line graph\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Line graph for Decision Tree\n",
    "plt.plot(range(1, 11), top_feature_performance_dict['Decision Tree'], marker='o', label='Decision Tree')\n",
    "\n",
    "# Line graph for Naive Bayes\n",
    "plt.plot(range(1, 11), top_feature_performance_dict['Naive Bayes'], marker='o', label='Naive Bayes')\n",
    "\n",
    "plt.xlabel('Number of Top Features')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Classifier Performance with Top Features')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126e058-6f7d-4a7c-a38f-ab20d72818e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
